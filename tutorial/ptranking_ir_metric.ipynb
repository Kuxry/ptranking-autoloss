{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Review on IR Metrics\n",
    "\n",
    "Given the predictions $\\mathbf{y}=f(\\mathbf{x})$ and the corresponding ground-truth\n",
    "labels $\\mathbf{y}^{*}$, we use $Y^{+}=\\{i|y_{i}^{*}>0\\}$ and $Y^{-}=\\{j|y_{j}^{*}=0\\}$ to represent the sets of relevant documents and non-relevant documents, respectively. We use $\\mathbf{b}^{*}=\\mathbb{I}\\{\\mathbf{y}^{*}>0\\}$ with $b_{j}^{*}=\\mathbb{I}\\{y_{j}^{*}>0\\}$\n",
    "to represent the binarized ground-truth, and the cumulative sum on\n",
    "$\\mathbf{b}^{*}$ is given as $B_{k}^{*}=\\sum_{j=1}^{k}b_{j}^{*}$. The scoring function $f$ induces\n",
    "a ranking $\\bar{\\mathbf{y}}$.\n",
    "The corresponding ground-truth labels are  $\\mathbf{y}^{**}$. Furthermore, we denote the binarized ground-truth labels as $\\mathbf{b}^{**}=\\mathbb{I}\\{\\mathbf{y}^{**}>0\\}$\n",
    "with $b_{j}^{**}=\\mathbb{I}\\{y_{j}^{**}>0\\}$, and the cumulative\n",
    "sum on $\\mathbf{b}^{**}$ is given as $B_{k}^{**}=\\sum_{j=1}^{k}b_{j}^{**}$.\n",
    "\n",
    "To evaluate the effectiveness of a scoring function, a number of IR\n",
    "metrics have been proposed to emphasize the items that are ranked\n",
    "at higher positions. In general, the IR metrics are computed\n",
    "based on the list of ground-truth labels $\\bar{\\mathbf{y}}$ induced by $f$. For example, the binary-relevance IR metrics measure the performance of\n",
    "a specific ranking model based on $\\mathbf{b}^{**}$, such as precision and AP. The graded-relevance IR metrics measure the performance of a specific ranking model based on $\\mathbf{y}^{**}$,\n",
    "such as nDCG and ERR.\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision@k measures the proportion\n",
    "of relevant documents retrieved at a given truncation position, which is defined as:\n",
    "\\begin{equation}\n",
    "Pre@k=\\frac{1}{k}\\sum_{j=1}^{k}b_{j}^{**}\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ denotes the truncation position.\n",
    "\n",
    "## Average Precision\n",
    "\n",
    "Different from Precision@k that does not take into account the position\n",
    "at which a document is ranked, Average Precision (AP) is a **rank-sensitive**\n",
    "metric, which builds upon Precision as follows:\n",
    "\\begin{equation}\n",
    "AP=\\frac{1}{|Y^{+}|}\\sum_{j}b_{j}^{**}\\times Pre@j\n",
    "\\end{equation}\n",
    "\n",
    "Then Mean Average Precision (MAP) is defined\n",
    "as the mean of AP scores over a set of queries.\n",
    "\n",
    "## Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "Normalized Discounted Cumulative Gain (nDCG) is a **graded-relevance\n",
    "rank-sensitive** metric. The discounted cumulative\n",
    "gain (DCG) of a ranked list is given as $DCG@k=\\sum_{j=1}^{k}\\frac{2^{y_{j}^{**}}-1}{\\log_{2}(j+1)}$,\n",
    "where $G_{j}=2^{y_{j}^{**}}-1$ is usually referred to as the gain\n",
    "value of the $j$-th document.\n",
    "We denote the maximum DCG value attained by the ideal ranking as $DCG^{*}$,\n",
    "then normalizing DCG with $DCG^{*}$ gives nDCG as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "nDCG@k=\\frac{DCG@k}{DCG^{*}@k}\n",
    "\\end{equation}\n",
    "\n",
    "## Expected Reciprocal Rank (ERR)\n",
    "\n",
    "Expected Reciprocal Rank (ERR) is another popular graded-relevance rank-sensitive metric. Let $Pr(j)$ be the relevance probability of the document at rank\n",
    "$j$. In accordance with the gain function for nDCG, the relevance\n",
    "probability is commonly calculated as $Pr(j)=\\frac{2^{y_{j}^{**}}-1}{2^{\\max(\\mathbf{y}^{**})}}$.\n",
    "ERR interprets the relevance probability as the probability that the\n",
    "user is satisfied with the document at a rank position. Thus the probability\n",
    "that the user is dissatisfied with the documents at ranks from $1$\n",
    "to $k$ is given as $Disp(1,k)=\\prod_{i=1}^{k}(1-Pr(i))$. ERR is\n",
    "then defined as\n",
    "\n",
    "\\begin{equation}\n",
    "ERR@k=\\sum_{j=1}^{k}\\frac{Disp(1,j-1)\\cdot Pr(j)}{j}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Let $ERR^{*}$ the maximum ERR value attained by the ideal ranking, we have the normalized ERR as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "nERR@k=\\frac{ERR@k}{ERR^{*}@k}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}